{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YT4DC5oNePJd",
    "outputId": "f7190788-901d-45fb-ac74-d33b479696bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: javalang in c:\\users\\mallk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: six in c:\\users\\mallk\\appdata\\roaming\\python\\python37\\site-packages (from javalang) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install javalang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H2G9wq3tl89g"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tqdm import tqdm\n",
    "import collections, glob\n",
    "import json, sys\n",
    "import random\n",
    "from create_data_corpus import *\n",
    "# import tokenization\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r507ZedReyu-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "0-ch5vv9l6ga",
    "outputId": "c54f22c1-7e69-4684-a709-d028a2a58c08"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7788\\1218925450.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#flags.DEFINE_string(\"input_file\", 'training_token_corpus.txt',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mallk\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\util\\module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m       \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.compat.v1.compat' has no attribute 'v1'"
     ]
    }
   ],
   "source": [
    "flags = tf.compat.v1.flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "#flags.DEFINE_string(\"input_file\", 'training_token_corpus.txt',\n",
    "#                    \"Input raw text file (or comma-separated list of files).\")\n",
    "\n",
    "#flags.DEFINE_string(\"input_type_file\", 'training_type_corpus.txt',\n",
    " #                   \"Input type raw text file (or comma-separated list of files).\")\n",
    "\n",
    "#flags.DEFINE_string(\n",
    "#    \"output_file\", 'training_token_type_instances.txt',\n",
    "#    \"Output TF example file (or comma-separated list of files).\")\n",
    "\n",
    "#flags.DEFINE_string(\"token_vocab_file\", 'vocab_token.txt',\n",
    "#                    \"The token vocabulary file that the BERT model was trained on.\")\n",
    "#\n",
    "#flags.DEFINE_string(\"type_vocab_file\", 'vocab_type.txt', \"The type vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_lower_case\", False, \"Whether to lower case the input text. Should be True for uncased models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_whole_word_mask\", False, \"Whether to use whole word masking rather than per-WordPiece masking.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_seq_length\", 128, \"Maximum sequence length.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_predictions_per_seq\", 30, \"Maximum number of masked LM predictions per sequence.\")\n",
    "\n",
    "flags.DEFINE_integer(\"random_seed\", 12345, \"Random seed for data generation.\")\n",
    "\n",
    "flags.DEFINE_integer(\"dupe_factor\", 5, \"Number of times to duplicate the input data (with different masks).\")\n",
    "\n",
    "flags.DEFINE_float(\"masked_lm_prob\", 0.15, \"Masked LM probability.\")\n",
    "\n",
    "flags.DEFINE_float(\"short_seq_prob\", 0.1, \"Probability of creating sequences which are shorter than the maximum length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZJ5i80kUmed",
    "outputId": "87dbfb28-03e7-4b21-a0f6-b18544db7984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iniciando\n",
      "dupe time: 1\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Create masked LM/next sentence masked_lm TF examples for BERT.\"\"\"\n",
    "\n",
    "class TrainingInstance(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, origin_tokens, segment_ids, masked_lm_positions, masked_lm_labels, masked_lm_types,\n",
    "                 is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.origin_tokens = origin_tokens\n",
    "        self.next_tokens = origin_tokens[1:] + ['[PAD]']\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "        self.masked_lm_types = masked_lm_types\n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        s += \"tokens: %s\\n\" % (\" \".join(\n",
    "            [printable_text(x) for x in self.tokens]))\n",
    "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "        s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "            [str(x) for x in self.masked_lm_positions]))\n",
    "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "            [printable_text(x) for x in self.masked_lm_labels]))\n",
    "        s += \"\\n\"\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def write_instance_to_example_files(instances, word2id, type_word2id, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_file, fname=None):\n",
    "    \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
    "    writers = []\n",
    "    writers.append(tf.python_io.TFRecordWriter(output_file))\n",
    "\n",
    "    writer_index = 0\n",
    "    total_written = 0\n",
    "    for (inst_index, instance) in tqdm(enumerate(instances)):\n",
    "\n",
    "        input_ids = file_to_id(word2id, instance.tokens)\n",
    "        lm_input_ids = file_to_id(word2id, instance.origin_tokens)\n",
    "        lm_target_ids = file_to_id(word2id, instance.next_tokens)\n",
    "        # input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = file_to_id(type_word2id, instance.masked_lm_types)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            lm_input_ids.append(0)\n",
    "            lm_target_ids.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        # assert len(input_type_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        masked_lm_positions = list(instance.masked_lm_positions)\n",
    "        masked_lm_ids = file_to_id(word2id, instance.masked_lm_labels)\n",
    "\n",
    "        # masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "        masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "        while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "            masked_lm_positions.append(0)\n",
    "            masked_lm_ids.append(0)\n",
    "            input_type_ids.append(0)\n",
    "            masked_lm_weights.append(0.0)\n",
    "\n",
    "        next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "        # for Language model, no mask tokens\n",
    "        features[\"lm_input_ids\"] = create_int_feature(lm_input_ids)\n",
    "        features[\"lm_target_ids\"] = create_int_feature(lm_target_ids)\n",
    "\n",
    "        features[\"input_type_ids\"] = create_int_feature(input_type_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "        features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "        features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "        features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "        features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "        writers[writer_index].write(tf_example.SerializeToString())\n",
    "        writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "        total_written += 1\n",
    "\n",
    "        if inst_index < 20:\n",
    "            # tf.logging.info(\"*** Example ***\")\n",
    "            # tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "            #     [tokenization.printable_text(x) for x in instance.tokens]))\n",
    "\n",
    "            for feature_name in features.keys():\n",
    "                feature = features[feature_name]\n",
    "                values = []\n",
    "                if feature.int64_list.value:\n",
    "                    values = feature.int64_list.value\n",
    "                elif feature.float_list.value:\n",
    "                    values = feature.float_list.value\n",
    "                #tf.logging.info(\n",
    "                #    \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "\n",
    "    for writer in writers:\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "def create_int_feature(values):\n",
    "    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "    return feature\n",
    "\n",
    "\n",
    "def create_float_feature(values):\n",
    "    feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "    return feature\n",
    "\n",
    "\n",
    "def create_training_instances(input_files, input_type_files, vocab, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    all_documents = [[]]\n",
    "    all_type_documents = [[]]\n",
    "\n",
    "    # Input file format:\n",
    "    # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "    # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "    # sentence boundaries for the \"next sentence prediction\" task).\n",
    "    # (2) Blank lines between documents. Document boundaries are needed so\n",
    "    # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "\n",
    "    with open(input_files, 'r', encoding='utf-8') as f:\n",
    "        tokendata = f.readlines()\n",
    "\n",
    "    with open(input_type_files, 'r', encoding='utf-8') as f:\n",
    "        typedata = f.readlines()\n",
    "\n",
    "    assert len(tokendata) == len(typedata)\n",
    "\n",
    "    for i in range(len(tokendata)):\n",
    "\n",
    "        tokenline = tokendata[i].strip()\n",
    "        typeline = typedata[i].strip()\n",
    "\n",
    "        if not tokenline:\n",
    "            all_documents.append([])\n",
    "            all_type_documents.append([])\n",
    "        else:\n",
    "            #tokens = json.loads(tokenline)\n",
    "            #types = json.loads(typeline)\n",
    "            tokens = [item.strip() for item in tokenline.split(\"\\x1f\")]\n",
    "            types = [item.strip() for item in typeline.split(\"\\x1f\")]\n",
    "            if tokens and types:\n",
    "                all_documents[-1].append(tokens)\n",
    "                all_type_documents[-1].append(types)\n",
    "\n",
    "\n",
    "    # Remove empty documents\n",
    "    all_documents = [x for x in all_documents if x ]\n",
    "    all_type_documents = [x for x in all_type_documents if x]\n",
    "\n",
    "    assert len(all_documents) == len(all_type_documents)\n",
    "\n",
    "    rng.seed(12345)\n",
    "    rng.shuffle(all_documents)\n",
    "    rng.seed(12345)\n",
    "    rng.shuffle(all_type_documents)\n",
    "\n",
    "    vocab_words = list(vocab.keys())\n",
    "    instances = []\n",
    "    for fac in range(dupe_factor):\n",
    "        print('dupe time: {}'.format(fac+1))\n",
    "        for document_index in range(len(all_documents)):\n",
    "            instances.extend(\n",
    "                create_instances_from_document(\n",
    "                    all_documents, all_type_documents, document_index, max_seq_length, short_seq_prob,\n",
    "                    masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "    rng.shuffle(instances)\n",
    "    return instances\n",
    "\n",
    "\n",
    "def create_instances_from_document(\n",
    "        all_documents, all_type_documents, document_index, max_seq_length, short_seq_prob,\n",
    "        masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "    document = all_documents[document_index]\n",
    "    type_document = all_type_documents[document_index]\n",
    "\n",
    "    # Account for [CLS], [SEP], [SEP]\n",
    "    max_num_tokens = max_seq_length - 3\n",
    "\n",
    "    # We *usually* want to fill up the entire sequence since we are padding\n",
    "    # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "    # computation. However, we *sometimes*\n",
    "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "    # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "    # The `target_seq_length` is just a rough target however, whereas\n",
    "    # `max_seq_length` is a hard limit.\n",
    "\n",
    "    target_seq_length = max_num_tokens\n",
    "    if rng.random() < short_seq_prob:\n",
    "        target_seq_length = rng.randint(2, max_num_tokens)\n",
    "\n",
    "    # We DON'T just concatenate all of the tokens from a document into a long\n",
    "    # sequence and choose an arbitrary split point because this would make the\n",
    "    # next sentence prediction task too easy. Instead, we split the input into\n",
    "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "    # input.\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_chunk_type = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(document):\n",
    "        segment = document[i]\n",
    "        type_segment = type_document[i]\n",
    "        current_chunk.append(segment)\n",
    "        current_chunk_type.append(type_segment)\n",
    "        current_length += len(segment)\n",
    "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end = 1\n",
    "\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "                tokens_a = []\n",
    "                types_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "                    types_a.extend(current_chunk_type[j])\n",
    "\n",
    "                tokens_b = []\n",
    "                types_b = []\n",
    "                # Random next\n",
    "                is_random_next = False\n",
    "\n",
    "                if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                    # This should rarely go for more than one iteration for large\n",
    "                    # corpora. However, just to be careful, we try to make sure that\n",
    "                    # the random document is not the same as the document\n",
    "                    # we're processing.\n",
    "                    for _ in range(10):\n",
    "                        random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "                        if random_document_index != document_index:\n",
    "                            break\n",
    "\n",
    "                    random_document = all_documents[random_document_index]\n",
    "                    random_type_document = all_type_documents[random_document_index]\n",
    "                    random_start = rng.randint(0, len(random_document) - 1)\n",
    "                    for j in range(random_start, len(random_document)):\n",
    "                        tokens_b.extend(random_document[j])\n",
    "                        types_b.extend(random_type_document[j])\n",
    "                        if len(tokens_b) >= target_b_length:\n",
    "                            break\n",
    "                    # We didn't actually use these segments so we \"put them back\" so\n",
    "                    # they don't go to waste.\n",
    "                    num_unused_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unused_segments\n",
    "                # Actual next\n",
    "                else:\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "                        types_b.extend(current_chunk_type[j])\n",
    "\n",
    "                truncate_seq_pair(tokens_a, tokens_b, types_a, types_b, max_num_tokens, rng)\n",
    "\n",
    "                assert len(tokens_a) >= 1\n",
    "                assert len(tokens_b) >= 1\n",
    "\n",
    "                tokens = []\n",
    "                types = []\n",
    "                segment_ids = []\n",
    "                tokens.append(\"[CLS]\")\n",
    "                types.append(\"[CLS]\")\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                for token in tokens_a:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "                for t in types_a:\n",
    "                    types.append(t)\n",
    "\n",
    "                tokens.append(\"[SEP]\")\n",
    "                types.append(\"[SEP]\")\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                for token in tokens_b:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(1)\n",
    "\n",
    "                for t in types_b:\n",
    "                    types.append(t)\n",
    "\n",
    "                tokens.append(\"[SEP]\")\n",
    "                types.append(\"[SEP]\")\n",
    "\n",
    "                segment_ids.append(1)\n",
    "\n",
    "                (output_tokens, masked_lm_positions,\n",
    "                 masked_lm_labels, masked_lm_types) = create_masked_lm_predictions(\n",
    "                    tokens, types, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "\n",
    "                instance = TrainingInstance(\n",
    "                    tokens=output_tokens,\n",
    "                    origin_tokens=tokens,\n",
    "                    segment_ids=segment_ids,\n",
    "                    is_random_next=is_random_next,\n",
    "                    masked_lm_positions=masked_lm_positions,\n",
    "                    masked_lm_labels=masked_lm_labels,\n",
    "                    masked_lm_types=masked_lm_types)\n",
    "                instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_chunk_type = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "\n",
    "    return instances\n",
    "\n",
    "\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\", \"type\"])\n",
    "\n",
    "def create_masked_lm_predictions(tokens, types, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    assert len(tokens) == len(types)\n",
    "    cand_indexes = []\n",
    "\n",
    "    for (i, token) in enumerate(tokens):\n",
    "\n",
    "        if token == \"[CLS]\" or token == \"[SEP]   \":\n",
    "            continue\n",
    "\n",
    "        # Whole Word Masking means that if we mask all of the wordpieces\n",
    "        # corresponding to an original word. When a word has been split into\n",
    "        # WordPieces, the first token does not have any marker and any subsequence\n",
    "        # tokens are prefixed with ##. So whenever we see the ## token, we\n",
    "        # append it to the previous set of word indexes.\n",
    "        #\n",
    "        # Note that Whole Word Masking does *not* change the training code\n",
    "        # at all -- we still predict each WordPiece independently, softmaxed\n",
    "        # over the entire vocabulary.\n",
    "        if (False and len(cand_indexes) >= 1 and\n",
    "                token.startswith(\"##\")):\n",
    "            cand_indexes[-1].append(i)\n",
    "        else:\n",
    "            cand_indexes.append(i)\n",
    "\n",
    "\n",
    "    output_tokens = list(tokens)\n",
    "\n",
    "    num_to_predict = min(max_predictions_per_seq,\n",
    "                         max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "    masked_lms = []\n",
    "\n",
    "    for index in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        if types[index] != '_':\n",
    "            masked_token = \"[MASK]\"\n",
    "            output_tokens[index] = masked_token\n",
    "            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index], type=types[index]))\n",
    "\n",
    "    assert len(masked_lms) <= num_to_predict\n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    masked_lm_types = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "        masked_lm_types.append(p.type)\n",
    "\n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels, masked_lm_types)\n",
    "\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, types_a, types_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "\n",
    "    while True:\n",
    "\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        trunc_types = types_a if len(types_a) > len(types_b) else types_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "            del trunc_types[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()\n",
    "            trunc_types.pop()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    #'./Desktop/Investigacion/CugLM/Data/Per-training'\n",
    "    #'./drive/MyDrive/Investigacion/tokens_vocab'\n",
    "    input_files = '../data/Per-training/token_java_pt.txt'\n",
    "    input_type_files = '../data/Per-training/type_java_pt.txt'\n",
    "\n",
    "    token_word2id, token_vocab_size = read_vocab('../data/Per-training/tokens_vocab')\n",
    "    type_word2id, type_vocab_size = read_vocab('../data/Per-training/types_vocab')\n",
    "    print(\"iniciando\")\n",
    "    rng = random.Random(12345)\n",
    "    instances = create_training_instances(\n",
    "        input_files, input_type_files, token_word2id,128, 2,\n",
    "        0.1, 0.15, 30,\n",
    "        rng)\n",
    "    print(\"creadas instancias!\")\n",
    "\n",
    "\n",
    "    output_file = '../data/pt-instances'\n",
    "    tf.logging.info(\"*** Writing to output files ***\")\n",
    "    tf.logging.info(\"  %s\", output_file)\n",
    "    print(\"escribiendo instancias\")\n",
    "    write_instance_to_example_files(instances, token_word2id, type_word2id, 128,\n",
    "                                    30, output_file)\n",
    "    print(\"instancias escritas!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # flags.mark_flag_as_required(\"input_file\")\n",
    "    # flags.mark_flag_as_required(\"input_type_file\")\n",
    "    # flags.mark_flag_as_required(\"output_file\")\n",
    "    # flags.mark_flag_as_required(\"token_vocab_file\")\n",
    "    # flags.mark_flag_as_required(\"type_vocab_file\")\n",
    "\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwMlh2axHUyx"
   },
   "outputs": [],
   "source": [
    "n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
